{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from transformers import pipeline\n",
    "from transformers import BertConfig\n",
    "from transformers import BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os.path\n",
    "from os import path\n",
    "\n",
    "from transformers import *\n",
    "from collections import Counter\n",
    "\n",
    "import math\n",
    "import csv\n",
    "import tqdm\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that PyTorch sees it\n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bert_model():\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.config = BertConfig(output_hidden_states=True)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(name)\n",
    "#         self.object = BertForMaskedLM.from_pretrained(name, config=self.config)\n",
    "        self.object = BertModel.from_pretrained(name,\n",
    "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/nlp/anaconda/main/anaconda3/envs/katezhou/lib/python3.6/site-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  \"Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 \"\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-large-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_base_cased= bert_model('bert-base-cased')\n",
    "bert_large_cased = bert_model('bert-large-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id(keyword, tokenizer):    \n",
    "    tokenized_text = tokenizer.tokenize(\"[CLS] \" + keyword + \" [SEP]\")\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    return indexed_tokens[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_last_4(token_embeddings):\n",
    "    token_vecs_cat = []\n",
    "\n",
    "    for token in token_embeddings:\n",
    "        #stack all into 2d array\n",
    "        cat_vec = torch.stack((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "        #take the average across columns\n",
    "        cat_vec = torch.mean(cat_vec, 0)\n",
    "        token_vecs_cat.append(cat_vec)\n",
    "    return token_vecs_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_word_embeddings_average_first_token(model, text, ids):    \n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = model.tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = model.tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    model.object.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.object(tokens_tensor, segments_tensors)\n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "    \n",
    "    #     WORD EMBEDDING\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "    token_vecs_average_last_4 = average_last_4(token_embeddings)\n",
    "\n",
    "    #find where keyword is\n",
    "    word_embedding = []\n",
    "    try:\n",
    "        #find the sequence\n",
    "        #take the embedding of the first item in the sequence\n",
    "        index = [(i, i+len(ids)) for i in range(len(indexed_tokens)) if indexed_tokens[i:i+len(ids)] == ids][0][0]\n",
    "        word_embedding = token_vecs_average_last_4[index]\n",
    "\n",
    "    except:\n",
    "        word_embedding = None\n",
    "        #dataset has Albanian instead of just Albania\n",
    "        print(\"Skip sentence\")\n",
    "        return None, None \n",
    "    \n",
    "    #SENTENCE EMBEDDING\n",
    "    token_vecs = hidden_states[-2][0]\n",
    "\n",
    "#     Calculate the average of all 22 token vectors.\n",
    "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "\n",
    "    return word_embedding, sentence_embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_word_embeddings_average_tokens(model, text, ids):\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = model.tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = model.tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    model.object.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.object(tokens_tensor, segments_tensors)\n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "    \n",
    "    #     WORD EMBEDDING\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "    token_vecs_average_last_4 = average_last_4(token_embeddings)\n",
    "\n",
    "    #find where keyword is\n",
    "    try:\n",
    "        index = [(i, i+len(ids)) for i in range(len(indexed_tokens)) if indexed_tokens[i:i+len(ids)] == ids][0][0]\n",
    "    except:\n",
    "        #dataset has Albanian instead of just Albania\n",
    "        print(\"Skip sentence\", text)\n",
    "        return None, None \n",
    "    \n",
    "    word_embedding = []\n",
    "    for x in range(index, index+len(ids)):\n",
    "        #concat all subtokens\n",
    "        word_embedding.append(token_vecs_average_last_4[x])\n",
    "    #stack them\n",
    "    word_embedding = torch.stack(word_embedding, dim=0)\n",
    "    #take the average of subtokens\n",
    "    word_embedding = torch.mean(word_embedding, 0)\n",
    "        \n",
    "    #SENTENCE EMBEDDING\n",
    "    token_vecs = hidden_states[-2][0]\n",
    "\n",
    "#     Calculate the average of all 22 token vectors.\n",
    "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "\n",
    "    return word_embedding, sentence_embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_analysis(model, data, indices, ids):\n",
    "    word_embeddings = []\n",
    "    sentence_embeddings = []\n",
    "    index_numbers = []\n",
    "    for n, row in data.iterrows():\n",
    "        word_embedding, sentence_embedding = extract_word_embeddings_average_tokens(model, row[0], ids)\n",
    "        if (word_embedding is not None):\n",
    "            word_embeddings.append(word_embedding)\n",
    "            sentence_embeddings.append(sentence_embedding)\n",
    "            index_numbers.append(indices[n])\n",
    "            #if you found one, then you are good and can move on\n",
    "#             break\n",
    "\n",
    "    return word_embeddings, sentence_embeddings, index_numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Method of Creating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(filename, data, model):\n",
    "    append_write = 'a' # make a new file if not\n",
    "\n",
    "    with open(filename, append_write, newline='') as f:\n",
    "        writer = csv.writer(f, delimiter=',')\n",
    "        #change to keyword tag at times\n",
    "        for name, group in data.groupby(['keyword']):\n",
    "            #when group is not the same as keyword\n",
    "            name_match = group['keyword'].values[0]\n",
    "            ids = get_id(name_match, model.tokenizer)\n",
    "            word_embeddings, sentence_embeddings, row_numbers = run_analysis(model, pd.DataFrame(group['sentence'].values), group['index'].values, ids)\n",
    "            for word_embedding, sentence_embedding, row_number in zip(word_embeddings, sentence_embeddings, row_numbers):\n",
    "                writer.writerow(word_embedding.tolist() + sentence_embedding.tolist() + [row_number] + [name] + [len(ids)])\n",
    "            print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sentence</th>\n",
       "      <th>keyword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I am from Afghanistan.</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>I live in Afghanistan.</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>I hope this January I will get to travel to Af...</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>I am interesting in traveling to Afghanistan.</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>My friend is from Afghanistan.</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3855</td>\n",
       "      <td>15</td>\n",
       "      <td>I never thought to visit Sao Tome and Principe...</td>\n",
       "      <td>Sao Tome and Principe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3856</td>\n",
       "      <td>16</td>\n",
       "      <td>The news says that Sao Tome and Principe is go...</td>\n",
       "      <td>Sao Tome and Principe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3857</td>\n",
       "      <td>17</td>\n",
       "      <td>The athlete from Sao Tome and Principe has jus...</td>\n",
       "      <td>Sao Tome and Principe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3858</td>\n",
       "      <td>18</td>\n",
       "      <td>The actress was born in Sao Tome and Principe ...</td>\n",
       "      <td>Sao Tome and Principe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3859</td>\n",
       "      <td>19</td>\n",
       "      <td>A number of fossils has been found in Sao Tome...</td>\n",
       "      <td>Sao Tome and Principe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3860 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                           sentence  \\\n",
       "0         0                             I am from Afghanistan.   \n",
       "1         1                             I live in Afghanistan.   \n",
       "2         2  I hope this January I will get to travel to Af...   \n",
       "3         3      I am interesting in traveling to Afghanistan.   \n",
       "4         4                     My friend is from Afghanistan.   \n",
       "...     ...                                                ...   \n",
       "3855     15  I never thought to visit Sao Tome and Principe...   \n",
       "3856     16  The news says that Sao Tome and Principe is go...   \n",
       "3857     17  The athlete from Sao Tome and Principe has jus...   \n",
       "3858     18  The actress was born in Sao Tome and Principe ...   \n",
       "3859     19  A number of fossils has been found in Sao Tome...   \n",
       "\n",
       "                    keyword  \n",
       "0               Afghanistan  \n",
       "1               Afghanistan  \n",
       "2               Afghanistan  \n",
       "3               Afghanistan  \n",
       "4               Afghanistan  \n",
       "...                     ...  \n",
       "3855  Sao Tome and Principe  \n",
       "3856  Sao Tome and Principe  \n",
       "3857  Sao Tome and Principe  \n",
       "3858  Sao Tome and Principe  \n",
       "3859  Sao Tome and Principe  \n",
       "\n",
       "[3860 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "a_sentences = pd.read_csv(\"../artificial_context/artifical_data.csv\", header=None)\n",
    "a_sentences.columns=['sentence']\n",
    "\n",
    "country_data = pd.read_csv(\"../country_metadata/un_countries_meta.csv\")\n",
    "country_data\n",
    "\n",
    "data = pd.DataFrame()\n",
    "for country in country_data['Name'].values:\n",
    "    temp = a_sentences.copy()\n",
    "    temp['keyword'] = [country] * len(temp)\n",
    "    temp['sentence'] = temp['sentence'].apply(lambda x: x.replace(\"COUNTRY\", country))\n",
    "    data = data.append(temp)\n",
    "data = data.reset_index()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Afghanistan\n",
      "Albania\n",
      "Algeria\n",
      "Andorra\n",
      "Angola\n",
      "Antigua and Barbuda\n",
      "Argentina\n",
      "Armenia\n",
      "Australia\n",
      "Austria\n",
      "Azerbaijan\n",
      "Bahamas\n",
      "Bahrain\n",
      "Bangladesh\n",
      "Barbados\n",
      "Belarus\n",
      "Belgium\n",
      "Belize\n",
      "Benin\n",
      "Bhutan\n",
      "Bolivia\n",
      "Bosnia and Herzegovina\n",
      "Botswana\n",
      "Brazil\n",
      "Brunei\n",
      "Bulgaria\n",
      "Burkina Faso\n",
      "Burundi\n",
      "Cambodia\n",
      "Cameroon\n",
      "Canada\n",
      "Cape Verde\n",
      "Central African Republic\n",
      "Chad\n",
      "Chile\n",
      "China\n",
      "Colombia\n",
      "Comoros\n",
      "Costa Rica\n",
      "Cote d'Ivoire\n",
      "Croatia\n",
      "Cuba\n",
      "Cyprus\n",
      "Czechia\n",
      "Democratic Republic of the Congo\n",
      "Denmark\n",
      "Djibouti\n",
      "Dominica\n",
      "Dominican Republic\n",
      "East Timor\n",
      "Ecuador\n",
      "Egypt\n",
      "El Salvador\n",
      "Equatorial Guinea\n",
      "Eritrea\n",
      "Estonia\n",
      "Eswatini\n",
      "Ethiopia\n",
      "Fiji\n",
      "Finland\n",
      "France\n",
      "Gabon\n",
      "Gambia\n",
      "Georgia\n",
      "Germany\n",
      "Ghana\n",
      "Greece\n",
      "Grenada\n",
      "Guatemala\n",
      "Guinea\n",
      "Guinea-Bissau\n",
      "Guyana\n",
      "Haiti\n",
      "Honduras\n",
      "Hungary\n",
      "Iceland\n",
      "India\n",
      "Indonesia\n",
      "Iran\n",
      "Iraq\n",
      "Ireland\n",
      "Israel\n",
      "Italy\n",
      "Jamaica\n",
      "Japan\n",
      "Jordan\n",
      "Kazakhstan\n",
      "Kenya\n",
      "Kiribati\n",
      "Kuwait\n",
      "Kyrgyzstan\n",
      "Laos\n",
      "Latvia\n",
      "Lebanon\n",
      "Lesotho\n",
      "Liberia\n",
      "Libya\n",
      "Liechtenstein\n",
      "Lithuania\n",
      "Luxembourg\n",
      "Madagascar\n",
      "Malawi\n",
      "Malaysia\n",
      "Maldives\n",
      "Mali\n",
      "Malta\n",
      "Marshall Islands\n",
      "Mauritania\n",
      "Mauritius\n",
      "Mexico\n",
      "Micronesia\n",
      "Moldova\n",
      "Monaco\n",
      "Mongolia\n",
      "Montenegro\n",
      "Morocco\n",
      "Mozambique\n",
      "Myanmar\n",
      "Namibia\n",
      "Nauru\n",
      "Nepal\n",
      "Netherlands\n",
      "New Zealand\n",
      "Nicaragua\n",
      "Niger\n",
      "Nigeria\n",
      "North Korea\n",
      "North Macedonia\n",
      "Norway\n",
      "Oman\n",
      "Pakistan\n",
      "Palau\n",
      "Panama\n",
      "Papua New Guinea\n",
      "Paraguay\n",
      "Peru\n",
      "Philippines\n",
      "Poland\n",
      "Portugal\n",
      "Qatar\n",
      "Republic of the Congo\n",
      "Romania\n",
      "Russia\n",
      "Rwanda\n",
      "Saint Kitts and Nevis\n",
      "Saint Lucia\n",
      "Saint Vincent and the Grenadines\n",
      "Samoa\n",
      "San Marino\n",
      "Sao Tome and Principe\n",
      "Saudi Arabia\n",
      "Senegal\n",
      "Serbia\n",
      "Seychelles\n",
      "Sierra Leone\n",
      "Singapore\n",
      "Slovakia\n",
      "Slovenia\n",
      "Solomon Islands\n",
      "Somalia\n",
      "South Africa\n",
      "South Korea\n",
      "South Sudan\n",
      "Spain\n",
      "Sri Lanka\n",
      "Sudan\n",
      "Suriname\n",
      "Sweden\n",
      "Switzerland\n",
      "Syria\n",
      "Tajikistan\n",
      "Tanzania\n",
      "Thailand\n",
      "Togo\n",
      "Tonga\n",
      "Trinidad and Tobago\n",
      "Tunisia\n",
      "Turkey\n",
      "Turkmenistan\n",
      "Tuvalu\n",
      "Uganda\n",
      "Ukraine\n",
      "United Arab Emirates\n",
      "United Kingdom\n",
      "United States\n",
      "Uruguay\n",
      "Uzbekistan\n",
      "Vanuatu\n",
      "Venezuela\n",
      "Vietnam\n",
      "Yemen\n",
      "Zambia\n",
      "Zimbabwe\n"
     ]
    }
   ],
   "source": [
    "create_embeddings(\"../embeddings/artificial_embeddings_bert_base_cased.csv\", data, bert_base_cased)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random sentences 30K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'country_sentences.csv' does not exist: b'country_sentences.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-c7c827eb106e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Read data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"country_sentences.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'keyword'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sentence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/katezhou/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/katezhou/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/katezhou/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/katezhou/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/katezhou/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'country_sentences.csv' does not exist: b'country_sentences.csv'"
     ]
    }
   ],
   "source": [
    "#Read data\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"country_sentences.csv\", header=None)\n",
    "data.columns = ['keyword', 'sentence']\n",
    "data = data.reset_index()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_embeddings(\"country_names_first_embeddings.csv\", data, bert_base_cased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
